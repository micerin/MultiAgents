"""
Advanced Multi-Critic System - é«˜çº§å¤šç»´åº¦æ‰¹è¯„å®¶ç³»ç»Ÿ
====================================================

ä¸€ä¸ªçœŸå®çš„ä»£ç å®¡æŸ¥ç³»ç»Ÿï¼ŒåŒ…å«ï¼š
1. å¤šä¸ªä¸“ä¸š Criticï¼ˆä»£ç è´¨é‡ã€å®‰å…¨æ€§ã€é£æ ¼ã€æ€§èƒ½ï¼‰
2. è¯„åˆ†èšåˆä¸å†²çªè§£å†³
3. è¿­ä»£æ”¹è¿›å¾ªç¯
4. äººå·¥ä»‹å…¥æœºåˆ¶

æ¶æ„å›¾ï¼š
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                             â”‚
                    â–¼                                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
              â”‚  WRITER  â”‚  ç”Ÿæˆ/ä¿®æ”¹ä»£ç                           â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                        â”‚
                   â”‚                                              â”‚
                   â–¼                                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
         â”‚   PARALLEL CRITICS  â”‚  å¹¶è¡Œè¯„å®¡                        â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚                                  â”‚
         â”‚  â”‚Code â”‚Sec  â”‚Styleâ”‚â”‚                                  â”‚
         â”‚  â”‚Qual â”‚urityâ”‚     â”‚â”‚                                  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚                                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
                   â”‚                                              â”‚
                   â–¼                                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
            â”‚ AGGREGATOR â”‚  æ±‡æ€»è¯„åˆ†ï¼Œè§£å†³å†²çª                     â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
                  â”‚                                               â”‚
                  â–¼                                               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
            â”‚  DECISION  â”‚â”€â”€â”€â”€â–ºâ”‚ HUMAN    â”‚ (å¯é€‰)                â”‚
            â”‚   MAKER    â”‚     â”‚ REVIEW   â”‚                       â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â”‚
                  â”‚                 â”‚                             â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
                  â”‚                                               â”‚
             Pass?â”‚                                               â”‚
                  â”‚ No â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  â”‚ Yes
                  â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   END    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import os
import sys
from typing import TypedDict, Annotated, Literal, Optional
from dataclasses import dataclass
import operator
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from dotenv import load_dotenv
load_dotenv()

from langgraph.graph import StateGraph, END
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage


# ==========================================
# é…ç½®
# ==========================================

@dataclass
class CriticConfig:
    """Critic ç³»ç»Ÿé…ç½®"""
    max_iterations: int = 3
    pass_threshold: float = 7.0  # æ€»åˆ† 10 åˆ†ï¼Œ7 åˆ†åŠæ ¼
    require_human_review: bool = False  # æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    human_review_threshold: float = 6.0  # ä½äºæ­¤åˆ†æ•°éœ€äººå·¥å®¡æ ¸


# ==========================================
# çŠ¶æ€å®šä¹‰
# ==========================================

@dataclass
class CriticScore:
    """å•ä¸ª Critic çš„è¯„åˆ†"""
    critic_name: str
    score: float  # 0-10
    feedback: str
    suggestions: list[str]
    passed: bool


class MultiCriticState(TypedDict):
    """å¤š Critic ç³»ç»ŸçŠ¶æ€"""
    # è¾“å…¥
    task: str
    code: str
    
    # Critic è¯„åˆ†
    critic_scores: Annotated[list[dict], operator.add]
    
    # èšåˆç»“æœ
    final_score: float
    aggregated_feedback: str
    conflicts: list[str]
    
    # æ§åˆ¶æµ
    iteration: int
    approved: bool
    needs_human_review: bool
    human_decision: Optional[str]
    
    # å†å²
    revision_history: Annotated[list[str], operator.add]


# ==========================================
# LLM åˆå§‹åŒ–
# ==========================================

def get_llm():
    """è·å– Azure OpenAI LLM"""
    return AzureChatOpenAI(
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
        temperature=0.3,
    )


# ==========================================
# Writer Node
# ==========================================

def writer_node(state: MultiCriticState) -> MultiCriticState:
    """ä»£ç ç”Ÿæˆ/ä¿®æ”¹èŠ‚ç‚¹"""
    print(f"\n{'='*60}")
    print(f"âœï¸  WRITER (Iteration {state['iteration'] + 1})")
    print('='*60)
    
    llm = get_llm()
    
    if state["iteration"] == 0:
        # é¦–æ¬¡ç”Ÿæˆ
        prompt = f"""You are an expert Python developer. Write clean, well-documented code.

Task: {state['task']}

Requirements:
1. Follow PEP 8 style guidelines
2. Add type hints
3. Include docstrings
4. Handle potential errors
5. Consider security best practices

Output ONLY the Python code, no explanations."""
        
        print(f"   ğŸ“‹ Task: {state['task']}")
        print("   ğŸ”„ Generating initial code...")
        
    else:
        # åŸºäºåé¦ˆä¿®æ”¹
        feedback = state.get("aggregated_feedback", "")
        prompt = f"""You are an expert Python developer. Revise the code based on feedback.

Original Task: {state['task']}

Current Code:
```python
{state['code']}
```

Feedback to address:
{feedback}

Revision Requirements:
1. Address ALL feedback points
2. Maintain existing functionality
3. Improve code quality

Output ONLY the revised Python code, no explanations."""
        
        print(f"   ğŸ“‹ Revising based on feedback...")
        print(f"   ğŸ“ Feedback summary: {feedback[:100]}...")
    
    response = llm.invoke([HumanMessage(content=prompt)])
    code = response.content
    
    # æ¸…ç†ä»£ç å—æ ‡è®°
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    
    print(f"   âœ… Code generated ({len(code)} chars)")
    
    return {
        "code": code,
        "iteration": state["iteration"] + 1,
        "revision_history": [f"Iteration {state['iteration'] + 1}: Generated/Revised code"]
    }


# ==========================================
# Critic Nodes
# ==========================================

def code_quality_critic(state: MultiCriticState) -> MultiCriticState:
    """ä»£ç è´¨é‡ Critic"""
    print("\n   ğŸ” Code Quality Critic evaluating...")
    
    llm = get_llm()
    
    prompt = f"""You are a code quality expert. Evaluate this Python code.

Code:
```python
{state['code']}
```

Evaluate on these criteria (score 0-10 for each):
1. Readability (clear naming, structure)
2. Maintainability (modularity, DRY)
3. Documentation (docstrings, comments)
4. Error Handling (exceptions, edge cases)
5. Type Hints (completeness, correctness)

Output JSON format:
{{
    "scores": {{"readability": X, "maintainability": X, "documentation": X, "error_handling": X, "type_hints": X}},
    "average_score": X.X,
    "feedback": "Overall assessment...",
    "suggestions": ["suggestion1", "suggestion2", ...]
}}"""

    response = llm.invoke([HumanMessage(content=prompt)])
    
    try:
        # è§£æ JSON
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        result = json.loads(content)
        score = result.get("average_score", 5.0)
        feedback = result.get("feedback", "No feedback")
        suggestions = result.get("suggestions", [])
    except:
        score = 5.0
        feedback = response.content[:200]
        suggestions = []
    
    print(f"      Score: {score}/10")
    
    return {
        "critic_scores": [{
            "critic_name": "Code Quality",
            "score": score,
            "feedback": feedback,
            "suggestions": suggestions,
            "passed": score >= 7.0
        }]
    }


def security_critic(state: MultiCriticState) -> MultiCriticState:
    """å®‰å…¨æ€§ Critic"""
    print("   ğŸ”’ Security Critic evaluating...")
    
    llm = get_llm()
    
    prompt = f"""You are a security expert. Analyze this Python code for security issues.

Code:
```python
{state['code']}
```

Check for:
1. Injection vulnerabilities (SQL, Command, etc.)
2. Hardcoded secrets/credentials
3. Insecure data handling
4. Input validation issues
5. Authentication/Authorization flaws

Output JSON format:
{{
    "security_score": X.X,
    "vulnerabilities_found": ["vuln1", "vuln2"],
    "risk_level": "low/medium/high/critical",
    "feedback": "Security assessment...",
    "suggestions": ["fix1", "fix2", ...]
}}"""

    response = llm.invoke([HumanMessage(content=prompt)])
    
    try:
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        result = json.loads(content)
        score = result.get("security_score", 5.0)
        feedback = result.get("feedback", "No feedback")
        suggestions = result.get("suggestions", [])
        risk = result.get("risk_level", "unknown")
    except:
        score = 5.0
        feedback = response.content[:200]
        suggestions = []
        risk = "unknown"
    
    print(f"      Score: {score}/10 (Risk: {risk})")
    
    return {
        "critic_scores": [{
            "critic_name": "Security",
            "score": score,
            "feedback": feedback,
            "suggestions": suggestions,
            "passed": score >= 7.0
        }]
    }


def style_critic(state: MultiCriticState) -> MultiCriticState:
    """ä»£ç é£æ ¼ Critic"""
    print("   ğŸ¨ Style Critic evaluating...")
    
    llm = get_llm()
    
    prompt = f"""You are a Python style expert (PEP 8). Review this code for style compliance.

Code:
```python
{state['code']}
```

Check for:
1. PEP 8 compliance (naming, spacing, line length)
2. Import organization
3. Code formatting consistency
4. Pythonic idioms usage
5. Clean code principles

Output JSON format:
{{
    "style_score": X.X,
    "pep8_issues": ["issue1", "issue2"],
    "feedback": "Style assessment...",
    "suggestions": ["improvement1", "improvement2", ...]
}}"""

    response = llm.invoke([HumanMessage(content=prompt)])
    
    try:
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        result = json.loads(content)
        score = result.get("style_score", 5.0)
        feedback = result.get("feedback", "No feedback")
        suggestions = result.get("suggestions", [])
    except:
        score = 5.0
        feedback = response.content[:200]
        suggestions = []
    
    print(f"      Score: {score}/10")
    
    return {
        "critic_scores": [{
            "critic_name": "Style",
            "score": score,
            "feedback": feedback,
            "suggestions": suggestions,
            "passed": score >= 7.0
        }]
    }


# ==========================================
# Aggregator Node
# ==========================================

def aggregator_node(state: MultiCriticState) -> MultiCriticState:
    """èšåˆæ‰€æœ‰ Critic è¯„åˆ†"""
    print(f"\n{'='*60}")
    print("ğŸ“Š AGGREGATOR")
    print('='*60)
    
    scores = state["critic_scores"]
    
    # è®¡ç®—åŠ æƒå¹³å‡åˆ†
    weights = {
        "Code Quality": 0.4,
        "Security": 0.35,
        "Style": 0.25
    }
    
    total_weight = 0
    weighted_sum = 0
    all_feedback = []
    all_suggestions = []
    conflicts = []
    
    print("\n   ğŸ“‹ Critic Scores:")
    for score_dict in scores:
        name = score_dict["critic_name"]
        score = score_dict["score"]
        weight = weights.get(name, 0.33)
        
        weighted_sum += score * weight
        total_weight += weight
        
        status = "âœ…" if score_dict["passed"] else "âŒ"
        print(f"      {status} {name}: {score}/10 (weight: {weight})")
        
        all_feedback.append(f"[{name}] {score_dict['feedback']}")
        all_suggestions.extend(score_dict.get("suggestions", []))
    
    final_score = weighted_sum / total_weight if total_weight > 0 else 0
    
    # æ£€æµ‹å†²çªï¼ˆä¸åŒ Critic æ„è§ç›¸å·®å¤ªå¤§ï¼‰
    score_values = [s["score"] for s in scores]
    if max(score_values) - min(score_values) > 3:
        conflicts.append(f"Large score variance: {min(score_values)}-{max(score_values)}")
    
    # èšåˆåé¦ˆ
    aggregated = f"""
Final Score: {final_score:.1f}/10

Feedback Summary:
{chr(10).join(all_feedback)}

Top Suggestions:
{chr(10).join(['- ' + s for s in all_suggestions[:5]])}
"""
    
    print(f"\n   ğŸ¯ Final Score: {final_score:.1f}/10")
    
    config = CriticConfig()
    needs_human = (
        config.require_human_review or 
        final_score < config.human_review_threshold or
        len(conflicts) > 0
    )
    
    return {
        "final_score": final_score,
        "aggregated_feedback": aggregated,
        "conflicts": conflicts,
        "needs_human_review": needs_human,
        "critic_scores": []  # é‡ç½®ä»¥é¿å…ç´¯ç§¯
    }


# ==========================================
# Decision Node
# ==========================================

def decision_node(state: MultiCriticState) -> MultiCriticState:
    """å†³ç­–èŠ‚ç‚¹"""
    print(f"\n{'='*60}")
    print("âš–ï¸  DECISION MAKER")
    print('='*60)
    
    config = CriticConfig()
    
    passed = state["final_score"] >= config.pass_threshold
    
    if passed:
        print(f"   âœ… APPROVED (Score: {state['final_score']:.1f} >= {config.pass_threshold})")
    else:
        print(f"   âŒ REJECTED (Score: {state['final_score']:.1f} < {config.pass_threshold})")
        if state["iteration"] >= config.max_iterations:
            print(f"   âš ï¸  Max iterations ({config.max_iterations}) reached")
            passed = True  # å¼ºåˆ¶é€šè¿‡ï¼Œé¿å…æ— é™å¾ªç¯
    
    return {
        "approved": passed
    }


# ==========================================
# Human Review Node (Optional)
# ==========================================

def human_review_node(state: MultiCriticState) -> MultiCriticState:
    """äººå·¥å®¡æ ¸èŠ‚ç‚¹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print(f"\n{'='*60}")
    print("ğŸ‘¤ HUMAN REVIEW")
    print('='*60)
    
    print(f"\n   Score: {state['final_score']:.1f}/10")
    print(f"   Conflicts: {state['conflicts']}")
    print("\n   [Simulating human review...]")
    
    # æ¨¡æ‹Ÿäººå·¥å†³ç­–
    # å®é™…åœºæ™¯ä¸­è¿™é‡Œä¼šæš‚åœç­‰å¾…çœŸäººè¾“å…¥
    if state["final_score"] >= 5.0:
        decision = "approve"
        print("   ğŸ‘ Human approved with minor concerns")
    else:
        decision = "reject"
        print("   ğŸ‘ Human requested revisions")
    
    return {
        "human_decision": decision,
        "approved": decision == "approve"
    }


# ==========================================
# Routing Functions
# ==========================================

def route_after_decision(state: MultiCriticState) -> Literal["human_review", "end", "writer"]:
    """å†³ç­–åçš„è·¯ç”±"""
    config = CriticConfig()
    
    if state["approved"]:
        return "end"
    
    if state["needs_human_review"]:
        return "human_review"
    
    if state["iteration"] >= config.max_iterations:
        return "end"
    
    return "writer"


def route_after_human(state: MultiCriticState) -> Literal["end", "writer"]:
    """äººå·¥å®¡æ ¸åçš„è·¯ç”±"""
    if state["approved"]:
        return "end"
    return "writer"


# ==========================================
# æ„å»ºå›¾
# ==========================================

def build_multi_critic_graph():
    """æ„å»ºå¤š Critic ç³»ç»Ÿå›¾"""
    
    workflow = StateGraph(MultiCriticState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("writer", writer_node)
    workflow.add_node("code_quality_critic", code_quality_critic)
    workflow.add_node("security_critic", security_critic)
    workflow.add_node("style_critic", style_critic)
    workflow.add_node("aggregator", aggregator_node)
    workflow.add_node("decision", decision_node)
    workflow.add_node("human_review", human_review_node)
    
    # è®¾ç½®å…¥å£
    workflow.set_entry_point("writer")
    
    # Writer â†’ å¹¶è¡Œ Critics
    # æ³¨æ„ï¼šLangGraph çš„"å¹¶è¡Œ"æ˜¯é€šè¿‡æ‰‡å‡ºå®ç°çš„
    workflow.add_edge("writer", "code_quality_critic")
    workflow.add_edge("writer", "security_critic")
    workflow.add_edge("writer", "style_critic")
    
    # Critics â†’ Aggregator
    workflow.add_edge("code_quality_critic", "aggregator")
    workflow.add_edge("security_critic", "aggregator")
    workflow.add_edge("style_critic", "aggregator")
    
    # Aggregator â†’ Decision
    workflow.add_edge("aggregator", "decision")
    
    # Decision â†’ æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "decision",
        route_after_decision,
        {
            "end": END,
            "human_review": "human_review",
            "writer": "writer"
        }
    )
    
    # Human Review â†’ æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "human_review",
        route_after_human,
        {
            "end": END,
            "writer": "writer"
        }
    )
    
    return workflow.compile()


# ==========================================
# Main
# ==========================================

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         ğŸ” Advanced Multi-Critic System                      â•‘
    â•‘                                                              â•‘
    â•‘    A production-grade code review system with:               â•‘
    â•‘    â€¢ Multiple specialized Critics (Quality, Security, Style) â•‘
    â•‘    â€¢ Weighted score aggregation                              â•‘
    â•‘    â€¢ Conflict detection                                      â•‘
    â•‘    â€¢ Iterative improvement loop                              â•‘
    â•‘    â€¢ Optional human review                                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ„å»ºå›¾
    app = build_multi_critic_graph()
    
    # æµ‹è¯•ä»»åŠ¡
    task = """Write a Python function that:
1. Connects to a database
2. Fetches user data by ID
3. Returns the user info as a dictionary
4. Handles errors gracefully"""

    print(f"ğŸ“‹ Task: {task}")
    print("\n" + "="*60)
    print("Starting Multi-Critic Review Process...")
    print("="*60)
    
    # åˆå§‹çŠ¶æ€
    initial_state: MultiCriticState = {
        "task": task,
        "code": "",
        "critic_scores": [],
        "final_score": 0.0,
        "aggregated_feedback": "",
        "conflicts": [],
        "iteration": 0,
        "approved": False,
        "needs_human_review": False,
        "human_decision": None,
        "revision_history": []
    }
    
    # è¿è¡Œ
    result = app.invoke(initial_state)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š FINAL RESULTS")
    print("="*60)
    
    print(f"\nâœ… Approved: {result['approved']}")
    print(f"ğŸ“ˆ Final Score: {result['final_score']:.1f}/10")
    print(f"ğŸ”„ Iterations: {result['iteration']}")
    
    print("\nğŸ“ Final Code:")
    print("-"*40)
    print(result["code"])
    print("-"*40)
    
    print("\nğŸ“œ Revision History:")
    for entry in result["revision_history"]:
        print(f"   â€¢ {entry}")


if __name__ == "__main__":
    main()
